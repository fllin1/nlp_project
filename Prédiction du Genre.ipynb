{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First name with sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstname = pd.read_csv('firstname_with_sex.csv')\n",
    "\n",
    "# On prépare les colonnes séparées ['firstname', 'male', 'female']\n",
    "new_columns = firstname['firstname;male;female'].str.split(';', expand=True)\n",
    "firstname[['firstname', 'male', 'female']] = new_columns\n",
    "\n",
    "# On enlève l'ancienne colonne\n",
    "firstname.drop(columns=['firstname;male;female'], inplace=True)\n",
    "firstname['male'] = firstname['male'].astype(int)\n",
    "firstname['female'] = firstname['female'].astype(int)\n",
    "\n",
    "firstname.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcriptions with sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = pd.read_csv('transcriptions_with_sex.csv')\n",
    "\n",
    "# On regarde à quoi ressemblent les données\n",
    "transcriptions = transcriptions[transcriptions['sex'] != 'ambigu']\n",
    "\n",
    "def extract_info_groundtruth(text):\n",
    "    patterns = {\n",
    "        'surname': r'surname:\\s*([^\\s]+)',\n",
    "        'firstname': r'firstname:\\s*([^\\s]+)',\n",
    "        'occupation': r'occupation:\\s*([^\\s]+)',\n",
    "        'link': r'link:\\s*([^\\s]+)',\n",
    "        'employer': r'employer:\\s*([^\\s]+)',\n",
    "        'age': r'age:\\s*(\\d+)',\n",
    "        'birth_date': r'birth_date:\\s*(\\d+)',\n",
    "        'lob': r'lob:\\s*([^\\s]+)'\n",
    "    }\n",
    "    results = {key: '' for key in patterns} \n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            results[key] = match.group(1)\n",
    "    return pd.Series(results) \n",
    "\n",
    "# Appliquer la fonction et créer les colonnes \n",
    "info_columns = ['surname', 'firstname', 'occupation', 'link', 'employer', 'age', 'birth_date', 'lob']\n",
    "transcriptions[info_columns] = transcriptions['groundtruth'].apply(extract_info_groundtruth)\n",
    "\n",
    "# Assurer que toutes les colonnes textuelles sont de type str\n",
    "for col in ['surname', 'firstname', 'occupation', 'link', 'employer', 'lob']:\n",
    "    transcriptions[col] = transcriptions[col].astype(str)\n",
    "\n",
    "# Assurer que les colonnes 'age' et 'birth_date' sont numériques\n",
    "transcriptions['age'] = pd.to_numeric(transcriptions['age'], errors='coerce')\n",
    "transcriptions['birth_date'] = pd.to_numeric(transcriptions['birth_date'], errors='coerce')\n",
    "\n",
    "# On enlève les paramètres qui sont probablement peu pertinents à la prédiction\n",
    "transcriptions.drop(['subject_line', 'groundtruth', 'prediction', 'surname', 'employer', 'lob'], axis=1, inplace=True)\n",
    "transcriptions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Présentation et description des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1 - Statistiques descriptives des données disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fichier firstname_with_sex.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution des prénoms : Nombre total de prénoms uniques, les prénoms les plus fréquents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie que chaque ligne corresponde à un nom\n",
    "print(\"Nombre total de prénoms uniques :\", firstname['firstname'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regardons les prénoms les plus populaires chez les hommes\n",
    "firstname.sort_values('male', ascending=False, ignore_index=True).head(5)[['firstname', 'male']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regardons les prénoms les plus populaires chez les femmes\n",
    "firstname.sort_values('female', ascending=False, ignore_index=True).head(5)[['firstname', 'female']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Répartition du sexe : Pourcentage de prénoms majoritairement masculins, féminins et neutres selon les fréquences données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du pourcentage pour chaque prénom par genre\n",
    "firstname['total'] = firstname['male'] + firstname['female']\n",
    "firstname['percent_male'] = (firstname['male'] / firstname['total']) * 100\n",
    "firstname['percent_female'] = (firstname['female'] / firstname['total']) * 100\n",
    "firstname.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme de la répartition des noms chez les hommes\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(firstname['percent_male'], bins=50, color='blue', alpha=0.7, label='Homme')\n",
    "plt.xlabel('Percentage')\n",
    "plt.ylabel('Frequence')\n",
    "plt.title('Distribution des prénoms chez les Hommes en %')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme de la répartition des noms chez les femmes\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(firstname['percent_female'], bins=50, color='red', alpha=0.7, label='Female')\n",
    "plt.xlabel('Percentage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Names by Gender Percentage')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fichier transcriptions_with_sex.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des sexes\n",
    "sex_counts = transcriptions['sex'].value_counts()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=sex_counts.index, y=sex_counts.values, palette='pastel')\n",
    "plt.title('Répartition par sexe')\n",
    "plt.ylabel('Nombre')\n",
    "plt.show()\n",
    "\n",
    "# Analyse des âges\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(transcriptions['age'], bins=50, color='purple')\n",
    "plt.title('Distribution des âges')\n",
    "plt.xlabel('Âge')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.show()\n",
    "\n",
    "# Analyse des professions\n",
    "occupation_counts = transcriptions['occupation'].value_counts().head(10)\n",
    "sns.set_palette('deep')\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=occupation_counts.values, y=occupation_counts.index, palette='viridis')\n",
    "plt.title('Top 10 des professions')\n",
    "plt.xlabel('Fréquence')\n",
    "plt.ylabel('Profession')\n",
    "plt.show()\n",
    "\n",
    "# Analyse des professions\n",
    "link_counts = transcriptions['link'].value_counts().head(10)\n",
    "sns.set_palette('deep')\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=link_counts.values, y=link_counts.index, palette='magma')\n",
    "plt.title('Top 10 des liens')\n",
    "plt.xlabel('Fréquence')\n",
    "plt.ylabel('Liens')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2 Estimation de la taille des données cibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Expérimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.1 - Protocole Expériemental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1.a - Classifier SKLearn with naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Préprocessing - Le modèle que l'on utilise prend en entrer une liste de chaînes de caractères qui contient toutes informations et les prédictions associées.\n",
    "\n",
    "Nous allons donc revenir avec un format de données textes similaires à \"groundtruth\" que l'on avait avant le processing de notre base de données, mais l'on aura tout de même retirer les informations peu pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code a été adaptée depuis la ressource officielle : https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(transcriptions):\n",
    "    texts = []\n",
    "    for i in range(len(transcriptions)):\n",
    "        firstname = transcriptions['firstname'].iloc[i]\n",
    "        occupation = transcriptions['occupation'].iloc[i]\n",
    "        link = transcriptions['link'].iloc[i]\n",
    "        age = transcriptions['age'].iloc[i]\n",
    "        birth_date = transcriptions['birth_date'].iloc[i]\n",
    "        texts.append(f\"firstname: {firstname}, occupation: {occupation}, link: {link}, age: {age}, birth_date: {birth_date}\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = get_texts(transcriptions)\n",
    "train_labels = (transcriptions['sex'] == 'femme').astype(int).to_list()\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=.20)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_texts)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_labels)\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(train_texts, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(test_texts)\n",
    "np.mean(predicted == test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1.b - Réseau de Neurones Récurrents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette approche, nous allons encoder les prénoms en séquences de caractères et utiliser un RNN pour prédire le sexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder les prénoms en indices de caractères\n",
    "char_to_index = {char: idx + 1 for idx, char in enumerate(set(''.join(transcriptions['firstname'])))}\n",
    "max_length = max(len(name) for name in transcriptions['firstname'])\n",
    "\n",
    "def encode_names(name):\n",
    "    encoded = [char_to_index[char] for char in name]\n",
    "    return encoded + [0] * (max_length - len(name))\n",
    "\n",
    "# Préparer les tensors\n",
    "X_rnn = torch.tensor([encode_names(name) for name in transcriptions['firstname']])\n",
    "y_rnn = torch.tensor(y.values)\n",
    "\n",
    "# Création du DataLoader\n",
    "dataset = TensorDataset(X_rnn, y_rnn)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Définir le modèle RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(char_to_index) + 1, embedding_dim=8)\n",
    "        self.rnn = nn.RNN(input_size=8, hidden_size=16, batch_first=True)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'appareil\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Création du jeu de données\n",
    "def encode_name(name):\n",
    "    return [ord(char) - ord('a') + 1 for char in name.lower() if 'a' <= char <= 'z']\n",
    "\n",
    "max_length = max(len(name) for name in transcriptions['firstname'])\n",
    "names_encoded = [encode_name(name) + [0] * (max_length - len(encode_name(name))) for name in transcriptions['firstname']]\n",
    "names_encoded = torch.tensor(names_encoded, dtype=torch.long)\n",
    "sex_encoded = torch.tensor(transcriptions['sex'].apply(lambda x: 1 if x == 'femme' else 0).values)\n",
    "\n",
    "# Division des données\n",
    "dataset = TensorDataset(names_encoded, sex_encoded)\n",
    "train_dataset, test_dataset = random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
    "\n",
    "\n",
    "def rnn (batch_size, num_epochs, lr):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Définition du modèle RNN\n",
    "    class NameRNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(NameRNN, self).__init__()\n",
    "            self.embedding = nn.Embedding(27, 8) \n",
    "            self.rnn = nn.GRU(8, 16, batch_first=True)\n",
    "            self.fc = nn.Linear(16, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.embedding(x)\n",
    "            x, _ = self.rnn(x)\n",
    "            x = self.fc(x[:, -1, :])\n",
    "            return x\n",
    "\n",
    "    model = NameRNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    def train_model(model, train_loader, criterion, optimizer, num_epochs=num_epochs):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "    # Tester le modèle\n",
    "    def evaluate_model(model, test_loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "    train_model(model, train_loader, criterion, optimizer)\n",
    "    return evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1.c - HuggingFace : Modèle BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcriptions = transcriptions.sample(frac = 1).reset_index(drop=True)\n",
    "# transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = len(transcriptions)\n",
    "transcriptions_train = transcriptions[n_obs//5 :]\n",
    "transcriptions_test = transcriptions[ : n_obs//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code utilisé a été adapté depuis ressource officielle pour l'utilisation des modèles d'HuggingFace: https://huggingface.co/transformers/v3.2.0/custom_datasets.html#sequence-classification-with-imdb-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts (transcriptions):\n",
    "    texts = []\n",
    "    for i in range(len(transcriptions)):\n",
    "        firstname = transcriptions['firstname'].iloc[i]\n",
    "        occupation = transcriptions['occupation'].iloc[i]\n",
    "        link = transcriptions['link'].iloc[i]\n",
    "        age = transcriptions['age'].iloc[i]\n",
    "        birth_date = transcriptions['birth_date'].iloc[i]\n",
    "        texts.append(f\"firstname: {firstname}, occupation: {occupation}, link: {link}, age: {age}, birth_date: {birth_date}\")\n",
    "    return texts\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} \n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            correct_predictions += torch.sum(predictions == labels).item()\n",
    "            total_predictions += len(labels)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def BERT (num_epochs, batch_size, wd, lr):\n",
    "\n",
    "    # Séparation des données en features et labels\n",
    "    train_texts, train_labels = get_texts(transcriptions_train), (transcriptions_train['sex'] == 'femme').astype(int).to_list()\n",
    "    test_texts, test_labels = get_texts(transcriptions_test), (transcriptions_test['sex'] == 'femme').astype(int).to_list()\n",
    "\n",
    "    # validation sets and train sets\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.20)\n",
    "\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "    train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "    val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "    test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=wd,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optim = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    \n",
    "    # Créer les DataLoaders pour les jeux d'entraînement, de validation et de test\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Calculer la précision sur le jeu d'entraînement\n",
    "    train_accuracy = compute_accuracy(model, train_loader)\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Calculer la précision sur le jeu de validation\n",
    "    val_accuracy = compute_accuracy(model, val_loader)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Calculer la précision sur le jeu de test\n",
    "    test_accuracy = compute_accuracy(model, test_loader)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "    return [train_accuracy, val_accuracy, test_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list = [3, 5, 10, 15]\n",
    "batch_size_list = [8, 16, 32, 64]\n",
    "wd_list = [0.1, 0.01, 0.001, 0.0001]\n",
    "lr_list = [1e-5, 2e-5, 5e-5, 1e-4]\n",
    "\n",
    "accuracy_list = []\n",
    "hyperparameters_list = []\n",
    "results_list = []\n",
    "\n",
    "for num_epochs in num_epochs_list:\n",
    "    for batch_size in batch_size_list:\n",
    "        for wd in wd_list:\n",
    "            for lr in lr_list:\n",
    "                hyperparameters = [num_epochs, batch_size, wd, lr]\n",
    "                accuracy = BERT(num_epochs, batch_size, wd, lr)\n",
    "\n",
    "                hyperparameters_list.append(hyperparameters)\n",
    "                accuracy_list.append(accuracy)\n",
    "                results_list.append(hyperparameters + accuracy)\n",
    "\n",
    "results_data = pd.DataFrame(results_list, columns=['epochs', 'batch', 'wd', 'lr', 'train_accuracy', 'val_accuracy', 'test_accuracy'])\n",
    "results_data.to_csv('resultats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2 - Analyse des résultats obtenus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultats = pd.read_csv('resultats.csv')\n",
    "df_resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy en fontion du epochs, les marges d'érreurs sont définies en fonctions \n",
    "# des différences entre les résultats lorsque les autres hyperparamètres sont modifiés\n",
    "sns.lineplot(data=df_resultats, x='epochs', y='val_accuracy', label='Validation accuracy')\n",
    "sns.lineplot(data=df_resultats, x='epochs', y='test_accuracy', label='Test accuracy')\n",
    "\n",
    "plt.xlabel('Nombre d\\'epochs')\n",
    "plt.ylabel('Précision')\n",
    "\n",
    "plt.title('Accuracy en fontion du nombre d\\'epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy en fontion du batch\n",
    "sns.lineplot(data=df_resultats, x='batch', y='val_accuracy', label='Validation accuracy')\n",
    "sns.lineplot(data=df_resultats, x='batch', y='test_accuracy', label='Test accuracy')\n",
    "\n",
    "plt.xlabel('Taille des Batchs')\n",
    "plt.ylabel('Précision')\n",
    "\n",
    "plt.title('Accuracy en fontion de la taille des batchs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy en fontion du weight decay\n",
    "sns.lineplot(data=df_resultats, x='wd', y='val_accuracy', label='Validation accuracy')\n",
    "sns.lineplot(data=df_resultats, x='wd', y='test_accuracy', label='Test accuracy')\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.xlabel('Weight Decay')\n",
    "plt.ylabel('Précision')\n",
    "\n",
    "plt.title('Accuracy en fontion du weight decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy en fontion du learning-rate\n",
    "sns.lineplot(data=df_resultats, x='lr', y='val_accuracy', label='Validation accuracy')\n",
    "sns.lineplot(data=df_resultats, x='lr', y='test_accuracy', label='Test accuracy')\n",
    "\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Précision')\n",
    "\n",
    "plt.title('Accuracy en fontion du learning-rate')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
